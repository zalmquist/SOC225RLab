---
title: "lab11: text as data"
subtitle: "Soc 225: Data & Society"
author: "[PUT YOUR NAME HERE]"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
---

# Agenda
- Introduction to text data
- Combine strings
- Pulling out part of strings
- Regular expressions
  - Basic matches
  - Anchors
- Text analysis with Twitter data


# 1: Text data (aka "strings")

Character vectors can contain rich, messy, unstructured data that we might use to produce categories or quantities. 

When we talk about manipulating character data in this way, we usually call the data a *string*. The `stringr` package contains tools for working with strings. `stringr` is a part of the tidyverse that isn't loaded by default. It contains some sample data we'll use, a list of names of fruit.

**Question 1.1: Load the `stringr` package and check out the list of `fruit` it contains by typing "fruit".**

```{r}
library(stringr)
```

# 2. Combining strings

`str_c` is our basic way of joining strings together: 

```{r}
str_c("pine", "apple")
```

You can join as many as you'd like: 

```{r}
str_c("pine", "apple", "s")
```

You can see that they're joined together with no space in between. If you want to change that, use the argument `sep`:

```{r}
str_c("crab", "apple", sep = " ")
```

`str_c` is *vectorized*; it will recycle strings, like so: 

```{r}
str_c(c("pine", "crab"), "apple")
```

The argument `collapse` will combine all of the input vectors at the very end: 

```{r}
str_c(c("pine", "crab"), "apple", collapse = " ")
```

**Question 2.1: Combine all of the fruit in `fruit` into a single string, delineated by commas. Do you need to use `sep` or `collapse`? Why?**

```{r}

```

# 3. Pulling out parts of strings

The basic idea here is that we take a **string** (or vector of strings) and apply some sort of **pattern** to it. There are different kinds of things we might want to get back out of this, depending on our goal. 

For instance, we might want 

- only the items that match the pattern (`str_subset`)
- TRUE or FALSE for every item (`str_detect`)
- the pattern itself for every item (`str_extract`)

This can be a handy way filter a data set or to create new variables out of messy text data.

Let's begin by looking at fruits starting with the letter "a". The "^" here means the beginning of a string. 

```{r}
str_subset(fruit, "^a")
```

What do we get back if we try `str_detect` with the same pattern?

```{r}
str_detect(fruit, "^a")
```

How could we use this? If we have a data frame of fruit, then `str_detect` works nicely with `filter`: 

```{r}
fruit_data <- tibble(fruit)

fruit_data %>%
  filter(str_detect(fruit, "^a"))
```

**Question 3.1: "$" indicates the *end* of a string. Find all the fruit names that end with "a". Do this with both `fruit` and `fruit_data`.**

```{r}

```

Let's pretend I'm a fruit marketer interested in the latest trends in fruit names. I can extract particular patterns of interest using `str_extract`. (Remember, "|" means "or".)

```{r}
str_extract(fruit, "fruit|berry|melon")
```

**Question 3.2: Make a bar chart using these extracted elements. You can add any other elements that you're interested in to the pattern.**
(Check the hints if you have troubles)

```{r}

```


## Exercises

Type `str_` into your console to see more of the tools `stringr` contains. These exercises are meant to familiarize you with a few of them. Other functions you might find particularly useful include `str_replace` (find a pattern and replace it with something else) and `str_to_lower` (turn everything to lower case). 

**Question 3.3: Substrings. `str_sub` extracts a part of a string based on position. Check out the help for it, and use it to extract the first letter of each `fruit`. Make a bar chart of initial letter frequencies.** 

(Remember, you can use categorical tools like fct_rev and fct_infreq to improve your chart.)

```{r}

```

**Question 3.4: Splitting strings. Try out `str_split` and `str_split_fixed` on the `best_coast` data below. What's the difference in the resulting data structures?** 

For a challenge, see if you can turn the city and state into separate columns of a data frame. 
(Check the hints if you have troubles)

```{r}
best_coast <- c("Seattle, WA", "Portland, OR", "San Francisco, CA", "Vancouver, BC") 


```

# 4. Matching patterns with regular expressions

## Basic matches
Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful.

To learn regular expressions, we’ll use `str_view()` and `str_view_all()`. These functions take a character vector and a regular expression, and show you how they match. We’ll start with very simple regular expressions and then gradually get more and more complicated. Once you’ve mastered pattern matching, you’ll learn how to apply those ideas with various `stringr` functions.

The simplest patterns match exact strings:

```{r}
x <- c("apple", "banana", "pear")
str_view(x, "an")
```

The next step up in complexity is `.`, which matches any character (except a newline):

```{r}
str_view(x, ".a.")
```

But if “.” matches any character, how do you match the character “.”? You need to use an “escape” to tell the regular expression you want to match it exactly, not use its special behavior. Like strings, regexps use the backslash, `\`, to escape special behavior. So to match an `.`, you need the regexp `\.`. Unfortunately this creates a problem. We use strings to represent regular expressions, and `\` is also used as an escape symbol in strings. So to create the regular expression `\.` we need the string "\\.".

```{r}
# To create the regular expression, we need \\
dot <- "\\."

# But the expression itself only contains one:
writeLines(dot)

# And this tells R to look for an explicit "."
str_view(c("abc", "a.c", "bef"), "a\\.c")
```

**Question 4.1. Find in the list of fruits patterns that match "en".**
```{r}
str_view(fruit, "en")
```

**Question 4.2. Explain why each of these strings don't match a `\`: `"\"`, `"\\"`, `"\\\"`. **


## Anchors

By default, regular expressions will match any part of a string. It’s often useful to anchor the regular expression so that it matches from the start or end of the string. You can use:

 - `^` to match the start of the string.
 - `$` to match the end of the string.
 
```{r}
x <- c("apple", "banana", "pear")
str_view(x, "^a")
```

```{r}
str_view(x, "a$")
```

To remember which is which, try this mnemonic from Evan Misshula: if you begin with power (`^`), you end up with money (`$`).

To force a regular expression to only match a complete string, anchor it with both `^` and `$`:

```{r}
x <- c("apple pie", "apple", "apple cake")
str_view(x, "apple")
```

```{r}
str_view(x, "^apple$")
```

**Question 4.3. In the list of fruits, find words that starts with "bl".**

```{r}
str_view(fruit, "^bl")
```

**Question 4.4. Given the corpus of common words in `stringr::words`, create regular expressions that find all words that starts with "y".**
```{r}

```

**Question 4.5. Given the corpus of common words in `stringr::words`, create regular expressions that find all words that ends with "x".**

```{r}

```

To go further with text data and *regular expressions*, you can start with http://stat545.com/block028_character-data.html and Chapter 14 of R for Data Science (http://r4ds.had.co.nz/strings.html) for a deeper dive into strings.

It's possible to go much further with analyzing unstructured text data. For instance, what if you want to analyze word counts or sentiments in a novel, or a bunch of tweets? The `tidytext` package is one framework for doing this. *Text Mining with R*, by Julia Silge and David Robinson, is a free online textbook for working with tidy text data, which you can find here: https://www.tidytextmining.com/

# 5. Text analysis with Twitter data

This last section takes you through an example of doing text analysis of Tweeter data with `stringr`. 

Tutorials on how to extract information from Tweeter, and more complex text analysis of Tweeter text could be found [here](https://mjalexander.github.io/social_media_workshop/). 

We'll be using a data set that contains about 125,000 tweets in Toronto. The tibble contains 90 columns, plus one additional column, which indicates whether or not the tweet mentions "vaccine" (or vaccinated, or vaccination). 

**Question 5.1. Load "toronto_tweets.Rda" file and `dplyr` package. Check column names.**
```{r}
# get(load("data/toronto_tweets.Rda"))
# library(dplyr)
# library(stringr)
```

We can filter our data set to only include tweets that mention "paid sick days".

```{r}
toronto_tweets %>% 
  filter(str_detect(text, "paid sick days")) %>% 
  select(text) %>% 
  head()
```

**Question 5.1. Filter the data set and search for key words you are interested in the text of Tweeter.**

**Question 5.2. Filter for key words we can use to take a rough guess of people who say they got vaccinated from 24-26 April.**

(Check the hints if you have troubles)
```{r}
people_vaccinated <- toronto_tweets %>% 
  # filter for Tweeter created between 24 and 27. Try day() to extract day from `create_at`
  # filter for key words that indicate people who got vaccinated
```

Now calculate how many unique users this corresponds to, and divide through by the total number of users, to get a rough rate of vaccination in Toronto:

```{r}
# check how many unique users said they got vaccinated during the period we are interested in
n_vax <- length(unique(people_vaccinated$user_id))
# check how many unique users in total
n_total <- toronto_tweets %>% 
  filter(day(created_at)>23&day(created_at)<27) %>% 
  summarize(length(unique(user_id))) %>% 
  pull()
# rate of vaccination 
n_vax/n_total*100
```

**Question 5.3. What do you get from the calculation? Based on [these](https://data.ontario.ca/dataset/covid-19-vaccine-data-in-ontario) data around 2% of the adult population were administered vaccines on these days. Is it different from your calculation? What do you think can explain the difference?**


# Hints

1.4 Start with `fruit_data` and use `mutate` to create a new variable.

1.6 Here's one way to do it. We haven't talked about the `purrr` package in the Tidyverse, but it's got some powerful tools like `map_chr`.

```{r}
tibble(best_coast) %>%
  mutate(split = str_split(best_coast, ", "), 
         city = map_chr(split, 1), 
         state = map_chr(split, 2)) %>%
  select(-split)
```

5.2. Here is an example:

```{r}
people_vaccinated <- toronto_tweets %>% 
  filter(day(created_at)>23&day(created_at)<27) %>% 
  filter((str_detect(text, "I got")|str_detect(text, "got my"))&
           str_detect(text, "vaccin*"))
```


# References 
Jenny Bryan, STAT 545, [Data wrangling, exploration, and analysis with R](http://stat545.com/block028_character-data.html)

[Text Mining with R](https://www.tidytextmining.com/)

R for Data Science, [chapter 14](http://r4ds.had.co.nz/strings.html)

Monica Alexander, [Social Media Data for Population Research](https://mjalexander.github.io/social_media_workshop/)